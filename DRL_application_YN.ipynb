{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "State Space = [p,h,b]. stock price p, stock shares h, remaining balance b\n",
    "\n",
    "Action Space = [sell, buy, hold]\n",
    "\n",
    "Reward : Change of pf value (depending on a,s,s')\n",
    "\n",
    "Policy\n",
    "\n",
    "Value function\n",
    "\n",
    "Action function\n",
    "\n",
    "- Reward Hypothesis? Maximize pf value. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env\n",
    "\n",
    "State Space : 9 stocks * 4\n",
    "\n",
    "1. Balance : 현금계좌 잔고\n",
    "\n",
    "2. Stock price : p\n",
    "\n",
    "3. Shares : 보유량\n",
    "\n",
    "4. MACD : SMA - LMA, signal = rolling by k days (closed price)\n",
    "\n",
    "(5) CCI : 주가가 MA에서 얼마나 떨어져 있는지 (과매수, 과매도 측정)\n",
    "\n",
    "(6) ADX : 추세의 강도를 측정\n",
    "\n",
    "(7) RSI\n",
    "\n",
    "Action Space : (2k+1)^9, k <= h_max (h_max는 사용자 설정 hp)\n",
    "\n",
    "{-k,...,-1,0,1,...,k}\n",
    "\n",
    "hmax : 한번에 살수있는 주식의 최대 share "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pandas import read_csv, set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "import math\n",
    "from numpy.random import choice\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  9 of 9 completed\n"
     ]
    }
   ],
   "source": [
    "ticker = ['XLB','XLE','XLF','XLI','XLK','XLP','XLU','XLV','XLY']\n",
    "stocks = yf.download(ticker, start = \"2013-01-01\", end = \"2023-05-09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XLB</th>\n",
       "      <th>XLE</th>\n",
       "      <th>XLF</th>\n",
       "      <th>XLI</th>\n",
       "      <th>XLK</th>\n",
       "      <th>XLP</th>\n",
       "      <th>XLU</th>\n",
       "      <th>XLV</th>\n",
       "      <th>XLY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>73.000000</td>\n",
       "      <td>13.704305</td>\n",
       "      <td>38.810001</td>\n",
       "      <td>29.809999</td>\n",
       "      <td>35.799999</td>\n",
       "      <td>35.560001</td>\n",
       "      <td>40.669998</td>\n",
       "      <td>48.430000</td>\n",
       "      <td>38.599998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>72.980003</td>\n",
       "      <td>13.688058</td>\n",
       "      <td>38.830002</td>\n",
       "      <td>29.620001</td>\n",
       "      <td>35.709999</td>\n",
       "      <td>35.560001</td>\n",
       "      <td>40.720001</td>\n",
       "      <td>48.549999</td>\n",
       "      <td>38.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>73.790001</td>\n",
       "      <td>13.850528</td>\n",
       "      <td>39.119999</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>35.810001</td>\n",
       "      <td>35.740002</td>\n",
       "      <td>40.900002</td>\n",
       "      <td>48.730000</td>\n",
       "      <td>38.720001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-07</th>\n",
       "      <td>73.220001</td>\n",
       "      <td>13.818034</td>\n",
       "      <td>38.950001</td>\n",
       "      <td>29.459999</td>\n",
       "      <td>35.570000</td>\n",
       "      <td>35.389999</td>\n",
       "      <td>41.029999</td>\n",
       "      <td>48.610001</td>\n",
       "      <td>38.630001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-08</th>\n",
       "      <td>72.980003</td>\n",
       "      <td>13.777417</td>\n",
       "      <td>38.650002</td>\n",
       "      <td>29.350000</td>\n",
       "      <td>35.470001</td>\n",
       "      <td>35.279999</td>\n",
       "      <td>41.040001</td>\n",
       "      <td>48.470001</td>\n",
       "      <td>38.740002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  XLB        XLE        XLF        XLI        XLK        XLP  \\\n",
       "Date                                                                           \n",
       "2013-01-02  73.000000  13.704305  38.810001  29.809999  35.799999  35.560001   \n",
       "2013-01-03  72.980003  13.688058  38.830002  29.620001  35.709999  35.560001   \n",
       "2013-01-04  73.790001  13.850528  39.119999  29.500000  35.810001  35.740002   \n",
       "2013-01-07  73.220001  13.818034  38.950001  29.459999  35.570000  35.389999   \n",
       "2013-01-08  72.980003  13.777417  38.650002  29.350000  35.470001  35.279999   \n",
       "\n",
       "                  XLU        XLV        XLY  \n",
       "Date                                         \n",
       "2013-01-02  40.669998  48.430000  38.599998  \n",
       "2013-01-03  40.720001  48.549999  38.590000  \n",
       "2013-01-04  40.900002  48.730000  38.720001  \n",
       "2013-01-07  41.029999  48.610001  38.630001  \n",
       "2013-01-08  41.040001  48.470001  38.740002  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close = stocks.iloc[:,10:19]\n",
    "close.columns=ticker\n",
    "close.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticker\n",
    "tic = []\n",
    "for n in range(len(close)):\n",
    "  for i in close.columns:\n",
    "    tic.append(i)\n",
    "\n",
    "# Date\n",
    "Date = np.repeat(close.index,9)\n",
    "\n",
    "# price\n",
    "price = []\n",
    "for r in range(len(close)):\n",
    "  for c in range(close.shape[1]):\n",
    "    price.append(close.iloc[r,c])\n",
    "\n",
    "# MACD\n",
    "def rolling(df,SMA, LMA, SIGNAL):\n",
    "  for i in range(df.shape[1]):\n",
    "    locals()[f\"each_ticker_{i}\"] = df.iloc[:,i].to_frame()\n",
    "    locals()[f\"each_ticker_{i}\"][f\"sma_{i}\"] = df.iloc[:,i].rolling(SMA).mean()\n",
    "    locals()[f\"each_ticker_{i}\"][f\"lma_{i}\"] = df.iloc[:,i].rolling(LMA).mean()\n",
    "    locals()[f\"each_ticker_{i}\"][\"macd\"] = locals()[f\"each_ticker_{i}\"][f'sma_{i}'] - locals()[f\"each_ticker_{i}\"][f'lma_{i}']\n",
    "    locals()[f\"each_ticker_{i}\"][\"signal\"] = locals()[f\"each_ticker_{i}\"].macd.rolling(SIGNAL).mean()\n",
    "    locals()[f\"each_ticker_{i}\"].dropna(inplace=True)\n",
    "\n",
    "  df_list = []\n",
    "  for j in range(df.shape[1]):\n",
    "    tmp = locals()[f\"each_ticker_{j}\"] # 이부분에서 코드 에러 났었음\n",
    "    df_list.append(tmp)\n",
    "\n",
    "  roll = pd.concat(df_list,axis=1)\n",
    "  \n",
    "\n",
    "  return roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate with random parameter setting\n",
    "df = rolling(close,30,60,15)\n",
    "\n",
    "# compatible for hyperparameter tuning\n",
    "# also considered memory usuage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Instantiation, create instantiated dataframe\n",
    "globals()['macd'] = []\n",
    "for i in range(len(df)):\n",
    "    tmp = list(df['macd'].iloc[i,:])\n",
    "    globals()['macd'].extend(tmp)\n",
    "\n",
    "globals()['signal'] = []\n",
    "for i in range(len(df)):\n",
    "    tmp = list(df['signal'].iloc[i,:])\n",
    "    globals()['signal'].extend(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "price = price[9*73:]\n",
    "tic = tic[:22788]\n",
    "Date = Date[9*73:]\n",
    "# macd\n",
    "# signal\n",
    "\n",
    "data = pd.DataFrame(zip(Date, price, tic, macd, signal),columns=['Date','price','tic','macd','signal'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Error Fix\n",
    "\n",
    "❓이 부분에서 코드 에러 났었음\n",
    "\n",
    "⭐ Before \n",
    "```\n",
    "df_list = []\n",
    "for j in range(df.shape[1]):\n",
    "  df_list.append(locals()[f\"each_ticker_{j}])\n",
    "```\n",
    "이럴때 결과값은 `['each_ticker_1','each_ticker_2',...]`를 반환했음. 근데 `pd.concat()`에 넣으려면 list element가 str이 아니라 변수 그 자체가 되야함. 따라서 `pd.concat`을 사용하지 못함. \n",
    "\n",
    "⭐ After\n",
    "```\n",
    "df_list = []\n",
    "for j in range(df.shape[1]):\n",
    "  tmp = locals()[f\"each_ticker_{j}\"] \n",
    "  df_list.append(tmp)\n",
    "```\n",
    "\n",
    "**리스트 안에 데이터프레임이 담겨있는 형태. 그러니 pd.concat()의 안에 필요한 것은, <리스트로 묶인 데이터프레임> 인 것이다. 따라서 그에 맞게 만들어준다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list = []\n",
    "# for j in range(close.shape[1]):\n",
    "#   df_list.append(f'each_ticker_{j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(close.shape[1]):\n",
    "#     globals()[f\"each_ticker_{i}\"] = close.iloc[:,i].to_frame()\n",
    "#     globals()[f\"each_ticker_{i}\"][f\"sma_{i}\"] = close.iloc[:,i].rolling(30).mean()\n",
    "#     globals()[f\"each_ticker_{i}\"][f\"lma_{i}\"] = close.iloc[:,i].rolling(60).mean()\n",
    "#     globals()[f\"each_ticker_{i}\"][\"macd\"] = globals()[f\"each_ticker_{i}\"][f'sma_{i}'] - globals()[f\"each_ticker_{i}\"][f'lma_{i}']\n",
    "#     globals()[f\"each_ticker_{i}\"][\"signal\"] = globals()[f\"each_ticker_{i}\"].macd.rolling(15).mean()\n",
    "#     globals()[f\"each_ticker_{i}\"].dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK_DIM=close.shape[1]-1\n",
    "INITIAL_ACCOUNT_BALANCE = 100000 # 10만 달러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class StockEnvTrain(gym.Env):\n",
    "\n",
    "    metadata = {\"render.modes\":[\"human\"]}\n",
    "\n",
    "    def __init__(self, data, day = 0, render_mode=None):\n",
    "      self.day = day \n",
    "      self.df = data\n",
    "\n",
    "      # Action Space\n",
    "      # normalize action space with stock_dim\n",
    "      self.action_space = spaces.Box(low=-1, high=1, shape=STOCK_DIM)\n",
    "\n",
    "      # State Space\n",
    "      # Shape : 36 : [current balance] + [prices 1-9] + [shares 1-9] + [MACD 1-9]\n",
    "      self.observation_space = spaces.Box(low=0, high=np.inf, shape=(36,))\n",
    "\n",
    "      # load data\n",
    "      self.data = self.df.loc[self.day,:]\n",
    "      self.terminal = False\n",
    "\n",
    "      # initialize state\n",
    "      self.state = [INITIAL_ACCOUNT_BALANCE]\n",
    "      self.data.macd.values.tolist()\n",
    "      \n",
    "      # initialize reward\n",
    "      self.reward = 0\n",
    "      self.cost = 0\n",
    "\n",
    "      # history of total balance change\n",
    "      self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "      self.rewards_memory=[]\n",
    "      self.trades = 0\n",
    "      self._seed()\n",
    "\n",
    "      assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "      self.render_mode = render_mode\n",
    "\n",
    "      \n",
    "      def _reset(self):\n",
    "         pass\n",
    "      def _render(self, mode='human', close=False):\n",
    "         pass\n",
    "      def _take_action(self, action):\n",
    "         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3 import TD3\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_A2C(env_train, model_name, timesteps=10000):\n",
    "    \"\"\"A2C Model\"\"\"\n",
    "    start = time.time()\n",
    "    model = A2C(\"MlpPolicy\", env_train, verbose=2)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"{config.TRAINED_MODEL_DIR}/{A2C_fin}\")\n",
    "    print(\"Training time (A2C): ', (end-start)/60,' minutes\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DDPG(env_train, model_name, timesteps=10000):\n",
    " \"\"\"DDPG model\"\"\"\n",
    " start = time.time()\n",
    " model = DDPG('MlpPolicy', env_train)\n",
    " model.learn(total_timesteps=timesteps)\n",
    " end = time.time()\n",
    " model.save(f'{config.TRAINED_MODEL_DIR}/{model_name}')\n",
    " print('Training time (DDPG): ', (end-start)/60,' minutes')\n",
    "       \n",
    " return model\n",
    "def train_PPO(env_train, model_name, timesteps=50000):\n",
    " \"\"\"PPO model\"\"\"\n",
    " start = time.time()\n",
    " model = PPO2('MlpPolicy', env_train)\n",
    " model.learn(total_timesteps=timesteps)\n",
    " end = time.time()\n",
    " model.save(f'{config.TRAINED_MODEL_DIR}/{model_name}')\n",
    " print('Training time (PPO): ', (end-start)/60,' minutes')\n",
    " return model\n",
    "\n",
    "def DRL_prediction(model, test_data, test_env, test_obs):\n",
    " \"\"\"make a prediction\"\"\"\n",
    " start = time.time()\n",
    " for i in range(len(test_data.index.unique())):\n",
    "   action, _states = model.predict(test_obs)\n",
    "   test_obs, rewards, dones, info = test_env.step(action)\n",
    "   # env_test.render()\n",
    " end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The environment is of type <class 'type'>, not a Gymnasium environment. In this case, we expect OpenAI Gym to be installed and the environment to be an OpenAI Gym environment.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_A2C(StockEnvTrain, A2C)\n",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m, in \u001b[0;36mtrain_A2C\u001b[1;34m(env_train, model_name, timesteps)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m\"\"\"A2C Model\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m A2C(\u001b[39m\"\u001b[39;49m\u001b[39mMlpPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m, env_train, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39mtimesteps)\n\u001b[0;32m      6\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:87\u001b[0m, in \u001b[0;36mA2C.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, rms_prop_eps, use_rms_prop, use_sde, sde_sample_freq, normalize_advantage, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     64\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     65\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m     _init_setup_model: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     86\u001b[0m ):\n\u001b[1;32m---> 87\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     88\u001b[0m         policy,\n\u001b[0;32m     89\u001b[0m         env,\n\u001b[0;32m     90\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m     91\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m     92\u001b[0m         gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[0;32m     93\u001b[0m         gae_lambda\u001b[39m=\u001b[39;49mgae_lambda,\n\u001b[0;32m     94\u001b[0m         ent_coef\u001b[39m=\u001b[39;49ment_coef,\n\u001b[0;32m     95\u001b[0m         vf_coef\u001b[39m=\u001b[39;49mvf_coef,\n\u001b[0;32m     96\u001b[0m         max_grad_norm\u001b[39m=\u001b[39;49mmax_grad_norm,\n\u001b[0;32m     97\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[0;32m     98\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[0;32m     99\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[0;32m    100\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m    101\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m    102\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    103\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    104\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    105\u001b[0m         _init_setup_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    106\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    107\u001b[0m             spaces\u001b[39m.\u001b[39;49mBox,\n\u001b[0;32m    108\u001b[0m             spaces\u001b[39m.\u001b[39;49mDiscrete,\n\u001b[0;32m    109\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiDiscrete,\n\u001b[0;32m    110\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiBinary,\n\u001b[0;32m    111\u001b[0m         ),\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_advantage \u001b[39m=\u001b[39m normalize_advantage\n\u001b[0;32m    116\u001b[0m     \u001b[39m# Update optimizer inside the policy if we want to use RMSProp\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39m# (original implementation) rather than Adam\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:81\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     59\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     60\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[39m.\u001b[39mSpace], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     80\u001b[0m ):\n\u001b[1;32m---> 81\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     82\u001b[0m         policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m     83\u001b[0m         env\u001b[39m=\u001b[39;49menv,\n\u001b[0;32m     84\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m     85\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m     86\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m     87\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m     88\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[0;32m     89\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[0;32m     90\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     91\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m     92\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[0;32m     93\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m     94\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49msupported_action_spaces,\n\u001b[0;32m     95\u001b[0m     )\n\u001b[0;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps \u001b[39m=\u001b[39m n_steps\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m=\u001b[39m gamma\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:169\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m env \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     env \u001b[39m=\u001b[39m maybe_make_env(env, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 169\u001b[0m     env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_env(env, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, monitor_wrapper)\n\u001b[0;32m    171\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobservation_space\n\u001b[0;32m    172\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:216\u001b[0m, in \u001b[0;36mBaseAlgorithm._wrap_env\u001b[1;34m(env, verbose, monitor_wrapper)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m\"\"\" \"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mWrap environment with the appropriate wrappers if needed.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39mFor instance, to have a vectorized environment\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39m:return: The wrapped environment.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(env, VecEnv):\n\u001b[0;32m    215\u001b[0m     \u001b[39m# Patch to support gym 0.21/0.26 and gymnasium\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     env \u001b[39m=\u001b[39m _patch_env(env)\n\u001b[0;32m    217\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_wrapped(env, Monitor) \u001b[39mand\u001b[39;00m monitor_wrapper:\n\u001b[0;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m verbose \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:33\u001b[0m, in \u001b[0;36m_patch_env\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m env\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m gym_installed \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(env, gym\u001b[39m.\u001b[39mEnv):\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe environment is of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(env)\u001b[39m}\u001b[39;00m\u001b[39m, not a Gymnasium \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menvironment. In this case, we expect OpenAI Gym to be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minstalled and the environment to be an OpenAI Gym environment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m     )\n\u001b[0;32m     39\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mshimmy\u001b[39;00m  \u001b[39m# pytype: disable=import-error\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The environment is of type <class 'type'>, not a Gymnasium environment. In this case, we expect OpenAI Gym to be installed and the environment to be an OpenAI Gym environment."
     ]
    }
   ],
   "source": [
    "train_A2C(StockEnvTrain, A2C)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단은 Environment 구축 중\n",
    "gym.Env로 상속했는데 왜 안되는지 의문\n",
    "Open AI gym 환경 customize하는 방법 리서치 하기\n",
    "\n",
    "optuna로 hyperparameter tuning해서 hmax, SMA, LMA, SIGNAL optimize 해볼수 있을듯\n",
    "\n",
    "transaction cost 구현 안되있음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfolio\n",
    "with pyfolio.plotting.plotting_context(font_scale=1.1):\n",
    " pyfolio.create_full_tear_sheet(returns = ensemble_strat,\n",
    " benchmark_rets=dow_strat, set_context=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
