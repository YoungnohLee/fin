{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4/bZEKN5aZL+HBDvckuPT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungnohLee/fin/blob/main/%5Bfin%5D_DRL_optuna_ideas%20and%20preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stockstats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJMeyu1m9Ees",
        "outputId": "49ae197d-782b-4cb6-f4eb-8e5bd73f01a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stockstats\n",
            "  Downloading stockstats-0.5.2-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from stockstats) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->stockstats) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->stockstats) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->stockstats) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.24.2->stockstats) (1.16.0)\n",
            "Installing collected packages: stockstats\n",
            "Successfully installed stockstats-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from stockstats import wrap\n",
        "\n",
        "# data = pd.read_csv('stock.csv')\n",
        "# df = wrap(data)"
      ],
      "metadata": {
        "id": "Wq4tLExA9i0F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "source code from `FinRL/finrl/config.py`\n",
        "\n",
        "`INDICATORS`부분에 원하는 보조지표를 추가하여 강화학습에 적용시켜 볼 수 있음. `config.py`를 rolling period 에 따른 함수로 만들어 load하면 rolling period 에 대해 `optuna`로 hyper parameter tuning 이 가능할 듯. 다만 데이터의 용량때문에 global 변수말고 lcoal 변수로 부른 다음에 함수 내에서 강화학습 알고리즘 돌리고 return 값만 반환하게 하는 방식을 택해보면 어떨까 하는 생각?\n",
        "(global 하게 해볼까?)"
      ],
      "metadata": {
        "id": "uD7piYHQ9773"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# directory\n",
        "from __future__ import annotations\n",
        "\n",
        "# customized config from finrl.config.py\n",
        "class config_rolling(sma=30, lma=60): \n",
        "    \n",
        "    DATA_SAVE_DIR = \"datasets\"\n",
        "    TRAINED_MODEL_DIR = \"trained_models\"\n",
        "    TENSORBOARD_LOG_DIR = \"tensorboard_log\"\n",
        "    RESULTS_DIR = \"results\"\n",
        "\n",
        "    # date format: '%Y-%m-%d'\n",
        "    TRAIN_START_DATE = \"2014-01-06\"  # bug fix: set Monday right, start date set 2014-01-01 ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 1658 and the array at index 1 has size 1657\n",
        "    TRAIN_END_DATE = \"2020-07-31\"\n",
        "\n",
        "    TEST_START_DATE = \"2020-08-01\"\n",
        "    TEST_END_DATE = \"2021-10-01\"\n",
        "\n",
        "    TRADE_START_DATE = \"2021-11-01\"\n",
        "    TRADE_END_DATE = \"2021-12-01\"\n",
        "\n",
        "    # stockstats technical indicator column names\n",
        "    # check https://pypi.org/project/stockstats/ for different names\n",
        "    def INDICATORS(sma=30, lma=60):\n",
        "      INDICATORS = [\n",
        "            \"macd\",\n",
        "            \"boll_ub\",\n",
        "            \"boll_lb\",\n",
        "            \"rsi_30\",\n",
        "            \"cci_30\",\n",
        "            \"dx_30\",\n",
        "            f\"close_{sma}_sma\",\n",
        "            f\"close_{lma}_sma\",\n",
        "      ]\n",
        "      return INDICATORS\n",
        "\n",
        "\n",
        "    # Model Parameters\n",
        "    A2C_PARAMS = {\"n_steps\": 5, \"ent_coef\": 0.01, \"learning_rate\": 0.0007}\n",
        "    PPO_PARAMS = {\n",
        "        \"n_steps\": 2048,\n",
        "        \"ent_coef\": 0.01,\n",
        "        \"learning_rate\": 0.00025,\n",
        "        \"batch_size\": 64,\n",
        "    }\n",
        "    DDPG_PARAMS = {\"batch_size\": 128, \"buffer_size\": 50000, \"learning_rate\": 0.001}\n",
        "    TD3_PARAMS = {\"batch_size\": 100, \"buffer_size\": 1000000, \"learning_rate\": 0.001}\n",
        "    SAC_PARAMS = {\n",
        "        \"batch_size\": 64,\n",
        "        \"buffer_size\": 100000,\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"learning_starts\": 100,\n",
        "        \"ent_coef\": \"auto_0.1\",\n",
        "    }\n",
        "    ERL_PARAMS = {\n",
        "        \"learning_rate\": 3e-5,\n",
        "        \"batch_size\": 2048,\n",
        "        \"gamma\": 0.985,\n",
        "        \"seed\": 312,\n",
        "        \"net_dimension\": 512,\n",
        "        \"target_step\": 5000,\n",
        "        \"eval_gap\": 30,\n",
        "        \"eval_times\": 64,  # bug fix:KeyError: 'eval_times' line 68, in get_model model.eval_times = model_kwargs[\"eval_times\"]\n",
        "    }\n",
        "    RLlib_PARAMS = {\"lr\": 5e-5, \"train_batch_size\": 500, \"gamma\": 0.99}\n",
        "\n",
        "\n",
        "    # Possible time zones\n",
        "    TIME_ZONE_SHANGHAI = \"Asia/Shanghai\"  # Hang Seng HSI, SSE, CSI\n",
        "    TIME_ZONE_USEASTERN = \"US/Eastern\"  # Dow, Nasdaq, SP\n",
        "    TIME_ZONE_PARIS = \"Europe/Paris\"  # CAC,\n",
        "    TIME_ZONE_BERLIN = \"Europe/Berlin\"  # DAX, TECDAX, MDAX, SDAX\n",
        "    TIME_ZONE_JAKARTA = \"Asia/Jakarta\"  # LQ45\n",
        "    TIME_ZONE_SELFDEFINED = \"xxx\"  # If neither of the above is your time zone, you should define it, and set USE_TIME_ZONE_SELFDEFINED 1.\n",
        "    USE_TIME_ZONE_SELFDEFINED = 0  # 0 (default) or 1 (use the self defined)\n",
        "\n",
        "    # parameters for data sources\n",
        "    ALPACA_API_KEY = \"xxx\"  # your ALPACA_API_KEY\n",
        "    ALPACA_API_SECRET = \"xxx\"  # your ALPACA_API_SECRET\n",
        "    ALPACA_API_BASE_URL = \"https://paper-api.alpaca.markets\"  # alpaca url\n",
        "    BINANCE_BASE_URL = \"https://data.binance.vision/\"  # binance url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "juMuiyOW9aAa",
        "outputId": "b397b6e7-e8e1-4b08-c6c2-3ec9c15fe32d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8c0dfba730b5>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# customized config from finrl.config.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mconfig_rolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mDATA_SAVE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"datasets\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: config_rolling.__init_subclass__() takes no keyword arguments"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before We Start...\n",
        "\n",
        "Concept Ideas from baseline\n",
        "\n",
        "- 기간마다 최고의 수익을 내는 알고리즘이 달랐음. 그래서 특정 기간마다 maximize reward 하는 알고리즘이 다를수 있다\n",
        "\n",
        "- 1. 그렇다면 전체 데이터에 대해 하나의 알고리즘으로 train하는게 아니라, 기간별로 다르게 train하면 좋지 않을까?\n",
        "\n",
        "- 2. 또한, 기간별로 같은 알고리즘을 쓸게 아니라, 기간별로 다른 알고리즘을 쓴다면 좋지 않을까?\n",
        "\n",
        "- 3. 이걸 더 확장해서 기간별로 데이터를 학습하되, 각각의 기간마다 여러 알고리즘을 앙상블 한다면, 일반 알고리즘을 선택하는 걸 넘어서 최적의 알고리즘이 쓰일 수 있지 않을까?\n",
        "\n",
        "- 그런데 과연 앙상블된 이 모델이, 실전에 적용된다고 했을때 최적의 일반화 성능을 낸다는 것을 보장할 수 있을까?\n",
        "\n",
        "- 만약 안된다면 우리는 최적의 일반화 성능을 도출해 내기 위해서 어떤 (앙상블)알고리즘을 택해야 하는걸까? 또한 어떤 방법(기간별 vs 전체 학습)을 적용해야 최고의 수익률을 내는데에 근접할 수 있을까?\n",
        "\n",
        "- 또한, 알고리즘의 근본이 다른데, 애초에 이것들을 앙상블 한다는 것이 가당키나 한걸까? 각각의 장단점을 지닌 모델들을 앙상블 한다는 것이, 각각의 장점을 살려서 단점을 보완하는 방식이 아니라, 각각의 단점을 살리지 못하고 단점을 극대화 시키는 방식이 아닐까? \n",
        "\\\n",
        "예를 들어서 Env를 근사하는 방식에 있어서 TD와 MC 방식을 살펴보았을때, TD는 부정확하지만 전체 데이터를 모으지 않고도 sequential 하게 데이터를 사용할 수 있고 random outlier에 error가 robust하다는 장점이 있지만 초반에 학습이 부정확하다는 단점이 있다. MC는 전체 데이터를 한번에 학습해서 Env를 추정하기 때문에 정확하다는 장점이 있지만 학습시간이 오래 걸리고 random outlier에 sensitive 하다는 단점이 있다. 이 둘을 앙상블 했을때, TD만 사용한다면 학습시간의 단축이라는 장점을 죽이고 MC가 모든 trajectiory를 모을때까지 기다려야 되는 것이다. 이런 상황에서 강화학습의 여러 알고리즘들을 앙상블 한다는 것이 좋다고 할 수 있을까?\n"
      ],
      "metadata": {
        "id": "Dpe55GstyvXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 또한 해당 FInRL의 `StockTradingEnv` class는 risk에 대한 대응을 다 팔고 가만히 있는 것으로 대응하는데, volatility가 높아지면 헷징을 하거나 단기투자로 바꾸는 등의 시도도 가능하지 않을까? 단순히 action_space를 `[ -hmax, hmax ] 로 하는게 아니라, 헷징이나 단기투자로 변환, 또는 공매도 포지션을 취하게 하는 등 여러가지 action이 가능하다면 훨씬 다양한 전략의 구현이 가능하지 않을까?\n"
      ],
      "metadata": {
        "id": "L1DgarB11hrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 해야되는 것\n",
        "\n",
        "- `finrl.config.py` 에 INDICATORS = [\n",
        "    \"macd\",\n",
        "    \"boll_ub\",\n",
        "    \"boll_lb\",\n",
        "    \"rsi_30\",\n",
        "    \"cci_30\",\n",
        "    \"dx_30\",\n",
        "    \"close_30_sma\",\n",
        "    \"close_60_sma\",\n",
        "]` 라고 정의되어 있음.\n",
        "\n",
        "- `f'close_{sma}_sma, close_{lma}_sma'` 꼴로 변환시켜서 sma, lma 를 optuna로 empirical 하게 변현시켜 rollin period 가 다른 데이터를 생성하려고 함.\n",
        "\n",
        "- 아래 코드를 보면 config.py에서 불러온 string element로 이루어진 INDICATORS 리스트를 FeatureEngineer module로 import 하고 있음.\n",
        "\n",
        "- FeatureEngineer module은 `finrl.meta.preprocessor.preprocessors`에서 불러옴.\n",
        "\n",
        "- 아래 `FeatureEngineer` class에서 `tech_indicator_list` 를 보면 `for indicator in self.tech_indicator_list:` 라고 되어있으면서, 그 아래에는 `stock = Sdf.retype(df.copy())` 에 `stock[stock.tic == unique_ticker[i]][indicator]` 로 되어있어서, stockstats/stockstats.py안에 **`StockDataFrame`** class 내부 `def retype(value):`에서 value에 `df.copy()`를 넣음으로써, 맨 마지막에 밑에서 4번째 줄에 있는 `ret = StockDataFrame(value)`에 `df.copy`가 들어가면서 indicator계산이 되는 형식임.\n",
        "\n",
        "- (질문) source code를 보니 `StockDataFrame(pd.DataFrame):`으로 구현이 되어있던데, 정작 안에서는 `pd.DataFrame`이 없었음. 그럼 input이 어떤 원리로 구현이 되는건지 잘 모르겠음. \n",
        "\n",
        "  optuna로 empirical 하게 구현하기 위해서는 rolling period를 def 나 class의 default argument로 할당해줘야 하는데 그렇게 하기 위해서 input을 어떻게 할당해주는 지 알 필요가 있다. 이 부분이 미해결로 남아있음."
      ],
      "metadata": {
        "id": "Y0Jcl9TrDUAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supported statistics/indicators are:\n",
        "\n",
        "- change (in percent)\n",
        "- delta\n",
        "- permutation (zero-based)\n",
        "- log return\n",
        "- max in range\n",
        "- min in range\n",
        "- middle = (close + high + low) / 3\n",
        "- compare: le, ge, lt, gt, eq, ne\n",
        "- count: both backward(c) and forward(fc)\n",
        "- cross: including upward cross and downward cross\n",
        "- SMA: Simple Moving Average\n",
        "- EMA: Exponential Moving Average\n",
        "- MSTD: Moving Standard Deviation\n",
        "- MVAR: Moving Variance\n",
        "- RSV: Raw Stochastic Value\n",
        "- RSI: Relative Strength Index\n",
        "- KDJ: Stochastic Oscillator\n",
        "- Bolling: Bollinger Band\n",
        "- MACD: Moving Average Convergence Divergence\n",
        "- CR: Energy Index (Intermediate Willingness Index)\n",
        "- WR: Williams Overbought/Oversold index\n",
        "- CCI: Commodity Channel Index\n",
        "- TR: True Range\n",
        "- ATR: Average True Range\n",
        "- DMA: Different of Moving Average (10, 50)\n",
        "- DMI: Directional Moving Index, including\n",
        "- +DI: Positive Directional Indicator\n",
        "- -DI: Negative Directional Indicator\n",
        "- ADX: Average Directional Movement Index\n",
        "- ADXR: Smoothed Moving Average of ADX\n",
        "- TRIX: Triple Exponential Moving Average\n",
        "- TEMA: Another Triple Exponential Moving Average\n",
        "- VR: Volume Variation Index\n",
        "- MFI: Money Flow Index\n",
        "- VWMA: Volume Weighted Moving Average\n",
        "- CHOP: Choppiness Index\n",
        "- KAMA: Kaufman's Adaptive Moving Average\n",
        "- PPO: Percentage Price Oscillator\n",
        "- StochRSI: Stochastic RSI\n",
        "- WT: LazyBear's Wave Trend\n",
        "- Supertrend: with the Upper Band and Lower Band"
      ],
      "metadata": {
        "id": "COeKA2HOLOew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stockstats/stockstats.py 내 class STockDataFrame(): 내 def retype 메소드 source_code\n",
        "    @staticmethod\n",
        "    def retype(value, index_column=None):\n",
        "        \"\"\" if the input is a `DataFrame`, convert it to this class.\n",
        "\n",
        "        :param index_column: name of the index column, default to `date`\n",
        "        :param value: value to convert\n",
        "        :return: this extended class\n",
        "        \"\"\"\n",
        "        if index_column is None:\n",
        "            index_column = 'date'\n",
        "\n",
        "        if isinstance(value, StockDataFrame):\n",
        "            return value\n",
        "        elif isinstance(value, pd.DataFrame):\n",
        "            name = value.columns.name\n",
        "            # use all lower case for column name\n",
        "            value.columns = map(lambda c: c.lower(), value.columns)\n",
        "\n",
        "            if index_column in value.columns:\n",
        "                value.set_index(index_column, inplace=True)\n",
        "            ret = StockDataFrame(value)\n",
        "            ret.columns.name = name\n",
        "            return ret\n",
        "        return value"
      ],
      "metadata": {
        "id": "4omQlKnaHI3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from finrl import config\n",
        "from finrl import config_tickers\n",
        "import os\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import (\n",
        "    DATA_SAVE_DIR,\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR,\n",
        "    INDICATORS,\n",
        "    TRAIN_START_DATE,\n",
        "    TRAIN_END_DATE,\n",
        "    TEST_START_DATE,\n",
        "    TEST_END_DATE,\n",
        "    TRADE_START_DATE,\n",
        "    TRADE_END_DATE,\n",
        ")\n",
        "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
      ],
      "metadata": {
        "id": "qRxMR7V_D6Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fe = FeatureEngineer(\n",
        "    use_technical_indicator = True,\n",
        "    tech_indicator_list = INDICATORS,\n",
        "    use_vix=True,\n",
        "    use_turbulence=True,\n",
        "    user_defined_feature=False\n",
        ")\n",
        "preprocessed = fe.preprocess_data(df)"
      ],
      "metadata": {
        "id": "RBpP9vG7D6WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split"
      ],
      "metadata": {
        "id": "DzhvgrquEfFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FinRL/finrl/meta/preprocessor/preprocessors.py\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import datetime\n",
        "from multiprocessing.sharedctypes import Value\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from stockstats import StockDataFrame as Sdf\n",
        "\n",
        "from finrl import config\n",
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "\n",
        "\n",
        "def load_dataset(*, file_name: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    load csv dataset from path\n",
        "    :return: (df) pandas dataframe\n",
        "    \"\"\"\n",
        "    # _data = pd.read_csv(f\"{config.DATASET_DIR}/{file_name}\")\n",
        "    _data = pd.read_csv(file_name)\n",
        "    return _data\n",
        "\n",
        "\n",
        "def data_split(df, start, end, target_date_col=\"date\"):\n",
        "    \"\"\"\n",
        "    split the dataset into training or testing using date\n",
        "    :param data: (df) pandas dataframe, start, end\n",
        "    :return: (df) pandas dataframe\n",
        "    \"\"\"\n",
        "    data = df[(df[target_date_col] >= start) & (df[target_date_col] < end)]\n",
        "    data = data.sort_values([target_date_col, \"tic\"], ignore_index=True)\n",
        "    data.index = data[target_date_col].factorize()[0]\n",
        "    return data\n",
        "\n",
        "\n",
        "def convert_to_datetime(time):\n",
        "    time_fmt = \"%Y-%m-%dT%H:%M:%S\"\n",
        "    if isinstance(time, str):\n",
        "        return datetime.datetime.strptime(time, time_fmt)\n",
        "\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"Provides methods for preprocessing the stock price data\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "        use_technical_indicator : boolean\n",
        "            we technical indicator or not\n",
        "        tech_indicator_list : list\n",
        "            a list of technical indicator names (modified from neofinrl_config.py)\n",
        "        use_turbulence : boolean\n",
        "            use turbulence index or not\n",
        "        user_defined_feature:boolean\n",
        "            use user defined features or not\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    preprocess_data()\n",
        "        main method to do the feature engineering\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        use_technical_indicator=True,\n",
        "        tech_indicator_list=config.INDICATORS,\n",
        "        use_vix=False,\n",
        "        use_turbulence=False,\n",
        "        user_defined_feature=False,\n",
        "    ):\n",
        "        self.use_technical_indicator = use_technical_indicator\n",
        "        self.tech_indicator_list = tech_indicator_list\n",
        "        self.use_vix = use_vix\n",
        "        self.use_turbulence = use_turbulence\n",
        "        self.user_defined_feature = user_defined_feature\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        \"\"\"main method to do the feature engineering\n",
        "        @:param config: source dataframe\n",
        "        @:return: a DataMatrices object\n",
        "        \"\"\"\n",
        "        # clean data\n",
        "        df = self.clean_data(df)\n",
        "\n",
        "        # add technical indicators using stockstats\n",
        "        if self.use_technical_indicator:\n",
        "            df = self.add_technical_indicator(df)\n",
        "            print(\"Successfully added technical indicators\")\n",
        "\n",
        "        # add vix for multiple stock\n",
        "        if self.use_vix:\n",
        "            df = self.add_vix(df)\n",
        "            print(\"Successfully added vix\")\n",
        "\n",
        "        # add turbulence index for multiple stock\n",
        "        if self.use_turbulence:\n",
        "            df = self.add_turbulence(df)\n",
        "            print(\"Successfully added turbulence index\")\n",
        "\n",
        "        # add user defined feature\n",
        "        if self.user_defined_feature:\n",
        "            df = self.add_user_defined_feature(df)\n",
        "            print(\"Successfully added user defined features\")\n",
        "\n",
        "        # fill the missing values at the beginning and the end\n",
        "        df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "        return df\n",
        "\n",
        "    def clean_data(self, data):\n",
        "        \"\"\"\n",
        "        clean the raw data\n",
        "        deal with missing values\n",
        "        reasons: stocks could be delisted, not incorporated at the time step\n",
        "        :param data: (df) pandas dataframe\n",
        "        :return: (df) pandas dataframe\n",
        "        \"\"\"\n",
        "        df = data.copy()\n",
        "        df = df.sort_values([\"date\", \"tic\"], ignore_index=True)\n",
        "        df.index = df.date.factorize()[0]\n",
        "        merged_closes = df.pivot_table(index=\"date\", columns=\"tic\", values=\"close\")\n",
        "        merged_closes = merged_closes.dropna(axis=1)\n",
        "        tics = merged_closes.columns\n",
        "        df = df[df.tic.isin(tics)]\n",
        "        # df = data.copy()\n",
        "        # list_ticker = df[\"tic\"].unique().tolist()\n",
        "        # only apply to daily level data, need to fix for minute level\n",
        "        # list_date = list(pd.date_range(df['date'].min(),df['date'].max()).astype(str))\n",
        "        # combination = list(itertools.product(list_date,list_ticker))\n",
        "\n",
        "        # df_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(df,on=[\"date\",\"tic\"],how=\"left\")\n",
        "        # df_full = df_full[df_full['date'].isin(df['date'])]\n",
        "        # df_full = df_full.sort_values(['date','tic'])\n",
        "        # df_full = df_full.fillna(0)\n",
        "        return df\n",
        "\n",
        "    def add_technical_indicator(self, data):\n",
        "        \"\"\"\n",
        "        calculate technical indicators\n",
        "        use stockstats package to add technical inidactors\n",
        "        :param data: (df) pandas dataframe\n",
        "        :return: (df) pandas dataframe\n",
        "        \"\"\"\n",
        "        df = data.copy()\n",
        "        df = df.sort_values(by=[\"tic\", \"date\"])\n",
        "        stock = Sdf.retype(df.copy())\n",
        "        unique_ticker = stock.tic.unique()\n",
        "\n",
        "        for indicator in self.tech_indicator_list:\n",
        "            indicator_df = pd.DataFrame()\n",
        "            for i in range(len(unique_ticker)):\n",
        "                try:\n",
        "                    temp_indicator = stock[stock.tic == unique_ticker[i]][indicator]\n",
        "                    temp_indicator = pd.DataFrame(temp_indicator)\n",
        "                    temp_indicator[\"tic\"] = unique_ticker[i]\n",
        "                    temp_indicator[\"date\"] = df[df.tic == unique_ticker[i]][\n",
        "                        \"date\"\n",
        "                    ].to_list()\n",
        "                    # indicator_df = indicator_df.append(\n",
        "                    #     temp_indicator, ignore_index=True\n",
        "                    # )\n",
        "                    indicator_df = pd.concat(\n",
        "                        [indicator_df, temp_indicator], axis=0, ignore_index=True\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "            df = df.merge(\n",
        "                indicator_df[[\"tic\", \"date\", indicator]], on=[\"tic\", \"date\"], how=\"left\"\n",
        "            )\n",
        "        df = df.sort_values(by=[\"date\", \"tic\"])\n",
        "        return df\n",
        "        # df = data.set_index(['date','tic']).sort_index()\n",
        "        # df = df.join(df.groupby(level=0, group_keys=False).apply(lambda x, y: Sdf.retype(x)[y], y=self.tech_indicator_list))\n",
        "        # return df.reset_index()\n",
        "\n",
        "    def add_user_defined_feature(self, data):\n",
        "        \"\"\"\n",
        "         add user defined features\n",
        "        :param data: (df) pandas dataframe\n",
        "        :return: (df) pandas dataframe\n",
        "        \"\"\"\n",
        "        df = data.copy()\n",
        "        df[\"daily_return\"] = df.close.pct_change(1)\n",
        "        # df['return_lag_1']=df.close.pct_change(2)\n",
        "        # df['return_lag_2']=df.close.pct_change(3)\n",
        "        # df['return_lag_3']=df.close.pct_change(4)\n",
        "        # df['return_lag_4']=df.close.pct_change(5)\n",
        "        return df\n",
        "\n",
        "    def add_vix(self, data):\n",
        "        \"\"\"\n",
        "        add vix from yahoo finance\n",
        "        :param data: (df) pandas dataframe\n",
        "        :return: (df) pandas dataframe\n",
        "        \"\"\"\n",
        "        df = data.copy()\n",
        "        df_vix = YahooDownloader(\n",
        "            start_date=df.date.min(), end_date=df.date.max(), ticker_list=[\"^VIX\"]\n",
        "        ).fetch_data()\n",
        "        vix = df_vix[[\"date\", \"close\"]]\n",
        "        vix.columns = [\"date\", \"vix\"]\n",
        "\n",
        "        df = df.merge(vix, on=\"date\")\n",
        "        df = df.sort_values([\"date\", \"tic\"]).reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "    def add_turbulence(self, data):\n",
        "        \"\"\"\n",
        "        add turbulence index from a precalcualted dataframe\n",
        "        :param data: (df) pandas dataframe\n",
        "        :return: (df) pandas dataframe\n",
        "        \"\"\"\n",
        "        df = data.copy()\n",
        "        turbulence_index = self.calculate_turbulence(df)\n",
        "        df = df.merge(turbulence_index, on=\"date\")\n",
        "        df = df.sort_values([\"date\", \"tic\"]).reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "    def calculate_turbulence(self, data):\n",
        "        \"\"\"calculate turbulence index based on dow 30\"\"\"\n",
        "        # can add other market assets\n",
        "        df = data.copy()\n",
        "        df_price_pivot = df.pivot(index=\"date\", columns=\"tic\", values=\"close\")\n",
        "        # use returns to calculate turbulence\n",
        "        df_price_pivot = df_price_pivot.pct_change()\n",
        "\n",
        "        unique_date = df.date.unique()\n",
        "        # start after a year\n",
        "        start = 252\n",
        "        turbulence_index = [0] * start\n",
        "        # turbulence_index = [0]\n",
        "        count = 0\n",
        "        for i in range(start, len(unique_date)):\n",
        "            current_price = df_price_pivot[df_price_pivot.index == unique_date[i]]\n",
        "            # use one year rolling window to calcualte covariance\n",
        "            hist_price = df_price_pivot[\n",
        "                (df_price_pivot.index < unique_date[i])\n",
        "                & (df_price_pivot.index >= unique_date[i - 252])\n",
        "            ]\n",
        "            # Drop tickers which has number missing values more than the \"oldest\" ticker\n",
        "            filtered_hist_price = hist_price.iloc[\n",
        "                hist_price.isna().sum().min() :\n",
        "            ].dropna(axis=1)\n",
        "\n",
        "            cov_temp = filtered_hist_price.cov()\n",
        "            current_temp = current_price[[x for x in filtered_hist_price]] - np.mean(\n",
        "                filtered_hist_price, axis=0\n",
        "            )\n",
        "            # cov_temp = hist_price.cov()\n",
        "            # current_temp=(current_price - np.mean(hist_price,axis=0))\n",
        "\n",
        "            temp = current_temp.values.dot(np.linalg.pinv(cov_temp)).dot(\n",
        "                current_temp.values.T\n",
        "            )\n",
        "            if temp > 0:\n",
        "                count += 1\n",
        "                if count > 2:\n",
        "                    turbulence_temp = temp[0][0]\n",
        "                else:\n",
        "                    # avoid large outlier because of the calculation just begins\n",
        "                    turbulence_temp = 0\n",
        "            else:\n",
        "                turbulence_temp = 0\n",
        "            turbulence_index.append(turbulence_temp)\n",
        "        try:\n",
        "            turbulence_index = pd.DataFrame(\n",
        "                {\"date\": df_price_pivot.index, \"turbulence\": turbulence_index}\n",
        "            )\n",
        "        except ValueError:\n",
        "            raise Exception(\"Turbulence information could not be added.\")\n",
        "        return turbulence_index"
      ],
      "metadata": {
        "id": "kQESoDGSFSPK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}